{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "073cfaa4",
   "metadata": {},
   "source": [
    "# Step 2: Analytical Robustness\n",
    "\n",
    "This notebook demonstrates how to derive origin–destination (OD) flows from the synthetic CDR datasets and assess their robustness to reasonable method choices. Following the exercise instructions, we compare OD matrices obtained under two different speed thresholds and quantify how sensitive the flows are to the movement definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0796e1c7",
   "metadata": {},
   "source": [
    "## 1 – Setup and Data Loading\n",
    "We start by importing the necessary libraries and loading the input datasets. This example uses `pandas` for tabular data, `geopandas` for spatial joins, and optionally Plotly for interactive maps.\n",
    "\n",
    "We load:\n",
    "* The synthetic CDR events (`events.csv`), diaries (`diaries.csv`) and cell tower information (`cells.csv`).\n",
    "* Population data and administrative boundaries (`BRA_adm2_pop.csv` and `BRA_adm2.shp`).\n",
    "\n",
    "These datasets will enable us to map user home locations to districts, compute OD flows, and compare the results of different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e8fadd",
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Attempt to import geopandas and plotly; if unavailable, raise a user‑friendly message\n",
    "try:\n",
    "    import geopandas as gpd\n",
    "except ImportError as e:\n",
    "    raise ImportError(\"geopandas is required for spatial operations. Install it via pip (pip install geopandas).\")\n",
    "\n",
    "try:\n",
    "    import plotly.express as px\n",
    "except ImportError as e:\n",
    "    raise ImportError(\"plotly is required for interactive maps. Install it via pip (pip install plotly).\")\n",
    "\n",
    "\n",
    "# File paths (adjust if you organise your files differently)\n",
    "events_fp = 'https://github.com/Flowminder/WB-GDF-Modern-Data-Workflows-Code-Tasks/raw/refs/heads/main/2.5.1_input_data/events.parquet'\n",
    "diaries_fp = 'https://github.com/Flowminder/WB-GDF-Modern-Data-Workflows-Code-Tasks/raw/refs/heads/main/2.5.1_input_data/diaries.csv'\n",
    "cells_fp = 'https://github.com/Flowminder/WB-GDF-Modern-Data-Workflows-Code-Tasks/raw/refs/heads/main/2.5.1_input_data/cells.csv'\n",
    "adm2_pop_fp = 'https://github.com/Flowminder/WB-GDF-Modern-Data-Workflows-Code-Tasks/raw/refs/heads/main/2.5.1_input_data/BRA_adm2_pop.csv'\n",
    "\n",
    "# Load CSV data\n",
    "events = pd.read_parquet(events_fp)\n",
    "diaries = pd.read_csv(diaries_fp)\n",
    "cells = pd.read_csv(cells_fp)\n",
    "adm2_pop = pd.read_csv(adm2_pop_fp)\n",
    "\n",
    "# Load only the Admin2 boundary from the shapefile zip using geopandas\n",
    "# The shapefile zip contains BRA_adm0.shp, BRA_adm1.shp, BRA_adm2.shp, BRA_adm3.shp.\n",
    "adm2_gdf = gpd.read_file(f\"https://github.com/Flowminder/WB-GDF-Modern-Data-Workflows-Code-Tasks/raw/refs/heads/main/2.5.1_input_data/BRA_adm2.gpkg\")\n",
    "# Ensure CRS is WGS84\n",
    "adm2_gdf = adm2_gdf.to_crs(epsg=4326)\n",
    "\n",
    "print(f'Loaded {len(events):,} CDR events, {len(diaries):,} diary records, {len(cells):,} cell towers and {len(adm2_gdf):,} Admin2 polygons.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e1da2c",
   "metadata": {},
   "source": [
    "## 2 – Map Home Locations to Admin 2\n",
    "The first step is to derive a home district for each user. We select rows from `diaries` where `stay_type == 'home'`, convert the coordinates into point geometries, and perform a spatial join with the Admin 2 boundaries.\n",
    "\n",
    "This mapping allows us to: (1) count the number of subscribers living in each district, and (2) filter to a target set of districts based on population or subscriber counts (e.g., São Paulo and surrounding areas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbafb707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import folium\n",
    "from folium import plugins\n",
    "import branca.colormap as cm\n",
    "\n",
    "# Filter to home locations only\n",
    "home_df = diaries[diaries['stay_type'] == 'home'].copy()\n",
    "home_df = home_df.dropna(subset=['longitude', 'latitude'])\n",
    "\n",
    "# Convert to GeoDataFrame (WGS84)\n",
    "home_gdf = gpd.GeoDataFrame(\n",
    "    home_df,\n",
    "    geometry=gpd.points_from_xy(home_df['longitude'], home_df['latitude']),\n",
    "    crs='EPSG:4326'\n",
    ")\n",
    "\n",
    "# Spatial join: assign each home point to an Admin2 polygon\n",
    "home_join = gpd.sjoin(\n",
    "    home_gdf,\n",
    "    adm2_gdf[['ID_2', 'NAME_2', 'geometry']],\n",
    "    how='left',\n",
    "    predicate='within'\n",
    ")\n",
    "\n",
    "# Count subscribers per Admin2 (unique user_id)\n",
    "home_counts = (\n",
    "    home_join\n",
    "    .groupby(['ID_2', 'NAME_2'])['user_id']\n",
    "    .nunique()\n",
    "    .reset_index(name='num_subscribers')\n",
    "    .sort_values('num_subscribers', ascending=False)\n",
    ")\n",
    "\n",
    "# Merge with synthetic census population (adm2_pop has columns: ID_2, NAME_2, pop_sum)\n",
    "adm2_pop_renamed = adm2_pop.rename(columns={'ID_2': 'ID_2', 'NAME_2': 'NAME_2'})\n",
    "home_counts = home_counts.merge(\n",
    "    adm2_pop_renamed[['ID_2', 'pop_sum']],\n",
    "    on='ID_2',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Compute coverage ratio (optional)\n",
    "home_counts['coverage_ratio'] = home_counts['num_subscribers'] / home_counts['pop_sum']\n",
    "\n",
    "# Select target Admin2 districts – e.g., top 12 by number of subscribers\n",
    "target_admin2 = home_counts.head(12)['ID_2'].tolist()\n",
    "print('Top 12 Admin2 districts by number of subscribers:')\n",
    "display(home_counts.head(12))\n",
    "\n",
    "# Prepare Admin2 polygons for Folium\n",
    "adm2_plot = adm2_gdf[['ID_2', 'NAME_2', 'geometry']].copy()\n",
    "adm2_plot['ID_2'] = adm2_plot['ID_2'].astype(int).astype(str)  # Remove .0\n",
    "\n",
    "# Count unique users per Admin2 (not all records)\n",
    "home_counts_for_map = (\n",
    "    home_join.dropna(subset=['ID_2'])\n",
    "    .groupby('ID_2')['user_id']\n",
    "    .nunique()\n",
    "    .reset_index(name='num_homes')\n",
    ")\n",
    "home_counts_for_map['ID_2'] = home_counts_for_map['ID_2'].astype(int).astype(str)\n",
    "\n",
    "# Merge home counts with Admin2 polygons\n",
    "adm2_plot = adm2_plot.merge(home_counts_for_map, on='ID_2', how='left')\n",
    "adm2_plot['num_homes'] = adm2_plot['num_homes'].fillna(0).astype(int)\n",
    "\n",
    "# Convert to GeoJSON for folium\n",
    "adm2_plot_json = json.loads(adm2_plot.to_json())\n",
    "\n",
    "# Get primary home location for each user (most frequent location)\n",
    "home_plot = home_join.dropna(subset=['ID_2']).copy()\n",
    "\n",
    "# For each user, find their most common home location\n",
    "user_primary_homes = []\n",
    "for user_id in home_plot['user_id'].unique():\n",
    "    user_homes = home_plot[home_plot['user_id'] == user_id]\n",
    "    # Group by lat/lon and count occurrences\n",
    "    location_counts = user_homes.groupby(['latitude', 'longitude', 'ID_2', 'NAME_2']).size().reset_index(name='count')\n",
    "    # Get the most frequent location\n",
    "    primary_home = location_counts.loc[location_counts['count'].idxmax()]\n",
    "    user_primary_homes.append({\n",
    "        'user_id': user_id,\n",
    "        'latitude': primary_home['latitude'],\n",
    "        'longitude': primary_home['longitude'],\n",
    "        'ID_2': primary_home['ID_2'],\n",
    "        'NAME_2': primary_home['NAME_2']\n",
    "    })\n",
    "\n",
    "home_plot = pd.DataFrame(user_primary_homes)\n",
    "home_plot['ID_2'] = home_plot['ID_2'].astype(int).astype(str)  # Remove .0\n",
    "\n",
    "print(f\"Total unique users with home locations: {len(home_plot)}\")\n",
    "\n",
    "# Create base map\n",
    "m = folium.Map(\n",
    "    location=[home_plot['latitude'].mean(), home_plot['longitude'].mean()],\n",
    "    zoom_start=6,\n",
    "    tiles='CartoDB positron'\n",
    ")\n",
    "\n",
    "# Create colormap for choropleth (only for values > 0)\n",
    "max_homes = adm2_plot[adm2_plot['num_homes'] > 0]['num_homes'].max()\n",
    "colormap = cm.LinearColormap(\n",
    "    colors=['#FFFFCC', '#FFEDA0', '#FED976', '#FEB24C', '#FD8D3C', '#FC4E2A', '#E31A1C', '#BD0026', '#800026'],\n",
    "    vmin=1,\n",
    "    vmax=max_homes,\n",
    "    caption='Number of Homes'\n",
    ")\n",
    "\n",
    "# Function to get fill color based on number of homes\n",
    "def get_fill_color(num_homes):\n",
    "    if num_homes < 15:\n",
    "        return '#E0E0E0'  # Gray for 0\n",
    "    else:\n",
    "        return colormap(num_homes)\n",
    "\n",
    "# Add choropleth layer\n",
    "folium.GeoJson(\n",
    "    adm2_plot_json,\n",
    "    name='Admin2 Districts',\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': get_fill_color(\n",
    "            adm2_plot[adm2_plot['ID_2'] == feature['properties']['ID_2']]['num_homes'].values[0]\n",
    "            if len(adm2_plot[adm2_plot['ID_2'] == feature['properties']['ID_2']]) > 0 else 0\n",
    "        ),\n",
    "        'color': 'black',\n",
    "        'weight': 1,\n",
    "        'fillOpacity': 0.6\n",
    "    },\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=['ID_2', 'NAME_2', 'num_homes'],\n",
    "        aliases=['ID:', 'District:', 'Homes:'],\n",
    "        localize=True\n",
    "    )\n",
    ").add_to(m)\n",
    "\n",
    "# Create a feature group for home points (can be toggled on/off)\n",
    "home_layer = folium.FeatureGroup(name='Home Locations', show=False)\n",
    "\n",
    "# Add home points\n",
    "for idx, row in home_plot.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        radius=0.5,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fillColor='blue',\n",
    "        fillOpacity=0.5,\n",
    "        popup=f\"User ID: {row['user_id']}\",\n",
    "        tooltip=f\"User ID: {row['user_id']}\"\n",
    "    ).add_to(home_layer)\n",
    "\n",
    "home_layer.add_to(m)\n",
    "\n",
    "# Add layer control to toggle home locations on/off\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# Add colormap legend\n",
    "colormap.add_to(m)\n",
    "\n",
    "m\n",
    "\n",
    "#Optionally save the map to an HTML file\n",
    "# m.save(\"home_location_map_15+.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ca7571",
   "metadata": {},
   "source": [
    "## 3 – Reuse Continuity Model Functions\n",
    "\n",
    "To derive OD flows from raw CDR events, we reuse functions from the **Continuity Models** notebook.  \n",
    "These functions perform the following steps:\n",
    "\n",
    "* **`add_coordinates_in_meters_to_events(events_df, cells_df)`** – merges event records with tower coordinates and projects them to a metric CRS (UTM) so that distances can be measured in metres.\n",
    "* **`add_speed_metric(df)`** – computes the speed (km/h) of movement between consecutive cell changes for each user.\n",
    "* **`determine_stay_move_from_speed(df, high_speed_threshold_kmh, low_speed_threshold_kmh)`** – classifies each event as `\"move\"` or `\"stay\"` based on two speed thresholds. Rows where speed > `high_speed_threshold_kmh` are marked as strong moves; contiguous rows with speed > `low_speed_threshold_kmh` are also included in the move segment.\n",
    "* **`collapse_stay_move(df)`** – collapses consecutive events of the same type into segments and computes a time-weighted centroid for stay segments.  The result is a table of stay and move segments with start/end times and coordinates.\n",
    "\n",
    "We copy these functions to here for convenience.  You can adjust the speed thresholds later to explore analytical robustness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27129124",
   "metadata": {
    "tags": [
     "functions"
    ]
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "def add_coordinates_in_meters_to_events(events_df: pd.DataFrame, cells_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge events with cell coordinates (longitude/latitude) and project them to a metric CRS (UTM).\n",
    "    Returns the events dataframe with additional columns: longitude, latitude, x, y (in metres).\n",
    "    \"\"\"\n",
    "    merged_df = events_df.merge(cells_df, on='cell_id', how='left')\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        merged_df,\n",
    "        geometry=gpd.points_from_xy(merged_df['longitude'], merged_df['latitude']),\n",
    "        crs='EPSG:4326'\n",
    "    )\n",
    "    # Project to UTM zone 23S (EPSG:31983) – appropriate for Brazil\n",
    "    gdf_projected = gdf.to_crs(epsg=31983)\n",
    "    gdf_projected['x'] = gdf_projected.geometry.x\n",
    "    gdf_projected['y'] = gdf_projected.geometry.y\n",
    "    return gdf_projected.drop(columns=['geometry'])\n",
    "\n",
    "def add_speed_metric(df: pd.DataFrame, group_by: str = 'user_id') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Computes backward-looking speed between consecutive cell changes for each user.\n",
    "    A constant speed is assigned within each stay/move segment.\n",
    "    \"\"\"\n",
    "    aux_df = df.sort_values([group_by, 'timestamp']).reset_index(drop=True)\n",
    "    aux_df['timestamp'] = pd.to_datetime(aux_df['timestamp'])\n",
    "    \n",
    "    def compute_speed(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        n = len(group)\n",
    "        if n == 0:\n",
    "            return group.assign(speed=np.nan)\n",
    "        x = group['x'].to_numpy(dtype=float)\n",
    "        y = group['y'].to_numpy(dtype=float)\n",
    "        ts = group['timestamp'].to_numpy('datetime64[ns]')\n",
    "        cell = group['cell_id'].to_numpy()\n",
    "        pos = np.arange(n)\n",
    "        # Identify the start index of each new cell segment\n",
    "        change_mask = np.r_[True, cell[1:] != cell[:-1]]\n",
    "        change_pos = pos[change_mask]\n",
    "        # For each index i, find the start of its current segment\n",
    "        idx_cur = np.searchsorted(change_pos, pos, side='right') - 1\n",
    "        cur_pos = np.full(n, -1, dtype=int)\n",
    "        mask_cur = idx_cur >= 0\n",
    "        cur_pos[mask_cur] = change_pos[idx_cur[mask_cur]]\n",
    "        # Previous segment start for each row\n",
    "        prev_idx = idx_cur - 1\n",
    "        prev_pos = np.full(n, -1, dtype=int)\n",
    "        mask_prev = prev_idx >= 0\n",
    "        prev_pos[mask_prev] = change_pos[prev_idx[mask_prev]]\n",
    "        # Compute speed only when both prev_pos and cur_pos are valid and different\n",
    "        speed = np.full(n, np.nan, dtype=float)\n",
    "        valid = (prev_pos != -1) & (cur_pos != -1) & (cur_pos != prev_pos)\n",
    "        if valid.any():\n",
    "            dt_h = (ts[cur_pos[valid]] - ts[prev_pos[valid]]) / np.timedelta64(1, 'h')\n",
    "            dist_km = np.hypot(\n",
    "                x[cur_pos[valid]] - x[prev_pos[valid]],\n",
    "                y[cur_pos[valid]] - y[prev_pos[valid]]\n",
    "            ) / 1000.0\n",
    "            speed_vals = np.full(valid.sum(), np.nan)\n",
    "            nonzero = dt_h > 0\n",
    "            speed_vals[nonzero] = dist_km[nonzero] / dt_h[nonzero]\n",
    "            speed[valid] = speed_vals\n",
    "        return group.assign(speed=speed)\n",
    "    aux_df = aux_df.groupby(group_by, group_keys=False).apply(compute_speed)\n",
    "    return aux_df\n",
    "\n",
    "def determine_stay_move_from_speed(df: pd.DataFrame, high_speed_threshold_kmh: float = 10.0, low_speed_threshold_kmh: float = 3.0, group_by: str = 'user_id') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Classify each event as 'move' or 'stay' based on speed thresholds.  \n",
    "    High threshold marks definite moves; contiguous rows with speed above the low threshold are also marked as moves.  \n",
    "    \"\"\"\n",
    "    def process_user(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        group = group.sort_values('timestamp').reset_index(drop=True)\n",
    "        group['speed'] = pd.to_numeric(group['speed'], errors='coerce')\n",
    "        # Initial classification: speed > high_speed → move\n",
    "        group['event_type'] = np.where(group['speed'] > high_speed_threshold_kmh, 'move', 'stay')\n",
    "        seed = group['event_type'].eq('move')\n",
    "        cand = group['speed'] > low_speed_threshold_kmh\n",
    "        mask = seed | cand\n",
    "        # Identify contiguous candidate islands\n",
    "        block_id = (mask.ne(mask.shift(fill_value=False))).cumsum()\n",
    "        block_id = block_id.where(mask)\n",
    "        # If any seed exists in the block, extend moves to include candidates\n",
    "        has_seed = seed.groupby(block_id).transform('any').fillna(False).astype(bool)\n",
    "        extend = cand & has_seed\n",
    "        group['event_type'] = np.where(seed | extend, 'move', 'stay')\n",
    "        return group\n",
    "    result = df.groupby(group_by, group_keys=False).apply(process_user)\n",
    "    return result\n",
    "\n",
    "def collapse_stay_move(df: pd.DataFrame, time_col: str = 'timestamp', type_col: str = 'event_type', lat_col: str = 'latitude', lon_col: str = 'longitude', group_by: str = 'user_id') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Collapse consecutive events of the same type into segments.  \n",
    "    For stay segments, compute a time-weighted centroid; for move segments, lat/lon may remain NaN.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    out[time_col] = pd.to_datetime(out[time_col], errors='coerce')\n",
    "    out = out.sort_values([group_by, time_col])\n",
    "    out['_seg_id'] = out.groupby(group_by)[type_col].transform(lambda s: (s != s.shift()).cumsum())\n",
    "    seg_bounds = out.groupby([group_by, '_seg_id']).agg(\n",
    "        event_type=(type_col, 'first'),\n",
    "        start_time=(time_col, 'first'),\n",
    "        end_time=(time_col, 'last'),\n",
    "    )\n",
    "    out = out.join(seg_bounds[['end_time']], on=[group_by, '_seg_id'], rsuffix='_seg')\n",
    "    out['_next_time'] = out.groupby([group_by, '_seg_id'])[time_col].shift(-1)\n",
    "    w = (out['_next_time'].fillna(out['end_time']) - out[time_col]).dt.total_seconds().clip(lower=0)\n",
    "    out['_w'] = w\n",
    "    out['_wlat'] = out['_w'] * out[lat_col]\n",
    "    out['_wlon'] = out['_w'] * out[lon_col]\n",
    "    agg = out.groupby([group_by, '_seg_id']).agg(\n",
    "        event_type=(type_col, 'first'),\n",
    "        start_time=(time_col, 'first'),\n",
    "        end_time=(time_col, 'last'),\n",
    "        w_sum=('_w', 'sum'),\n",
    "        wlat_sum=('_wlat', 'sum'),\n",
    "        wlon_sum=('_wlon', 'sum'),\n",
    "        lat_mean=(lat_col, 'mean'),\n",
    "        lon_mean=(lon_col, 'mean'),\n",
    "    )\n",
    "    agg['duration_s'] = (agg['end_time'] - agg['start_time']).dt.total_seconds()\n",
    "    stay_mask = agg['event_type'].eq('stay')\n",
    "    has_weight = agg['w_sum'] > 0\n",
    "    agg['latitude'] = np.where(stay_mask & has_weight, agg['wlat_sum'] / agg['w_sum'], np.where(stay_mask, agg['lat_mean'], np.nan))\n",
    "    agg['longitude'] = np.where(stay_mask & has_weight, agg['wlon_sum'] / agg['w_sum'], np.where(stay_mask, agg['lon_mean'], np.nan))\n",
    "    result = (\n",
    "        agg.reset_index(level='_seg_id', drop=True).reset_index()[\n",
    "            [group_by, 'event_type', 'start_time', 'end_time', 'duration_s', 'latitude', 'longitude']\n",
    "        ].sort_values([group_by, 'start_time']).reset_index(drop=True)\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def map_points_to_admin2(points_df: pd.DataFrame, adm2_gdf: gpd.GeoDataFrame, lat_col: str = 'latitude', lon_col: str = 'longitude') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assign each point in `points_df` to an Admin2 polygon using a spatial join.  \n",
    "    Returns a copy of the input DataFrame with added columns `GID_2` and `NAME_2`.  \n",
    "    \"\"\"\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        points_df,\n",
    "        geometry=gpd.points_from_xy(points_df[lon_col], points_df[lat_col]),\n",
    "        crs='EPSG:4326'\n",
    "    )\n",
    "    join = gpd.sjoin(gdf, adm2_gdf[['ID_2', 'NAME_2', 'geometry']], how='left', predicate='within')\n",
    "    # Return DataFrame without geometry\n",
    "    return join.drop(columns=['geometry'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aace892b",
   "metadata": {},
   "source": [
    "## 4 – Compute OD Flows for Methods A and B\n",
    "We now compute OD flows using the functions defined above. The overall workflow is:\n",
    "1. **Filter users whose home is in the target Admin 2 set.**\n",
    "2. **For each method, compute speeds between consecutive events, classify stay vs move using `high_speed` and `low_speed` thresholds, and collapse segments into trips.**\n",
    "3. **Assign each origin and destination of a trip to an Admin 2 district via spatial join, then count trips for each OD pair.**\n",
    "\n",
    "In this exercise we compare two specific methods:\n",
    "* **Method A** – baseline: `high_speed = 10 km/h`, `low_speed = 3 km/h`.\n",
    "* **Method B** – alternative: `high_speed = 20 km/h`, `low_speed = 3 km/h`.\n",
    "\n",
    "After computing the flows for both methods, we merge the tables by origin–destination pair, compute absolute and relative differences, and identify OD pairs where the **absolute relative difference** (|Flow_B – Flow_A| / Flow_A) exceeds a threshold (e.g., 30 %)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e892ad",
   "metadata": {
    "tags": [
     "od-helper"
    ]
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "def compute_od_flows(events_df: pd.DataFrame,\n",
    "                     cells_df: pd.DataFrame,\n",
    "                     target_user_ids: List[int],\n",
    "                     high_speed: float,\n",
    "                     low_speed: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute origin–destination flows at Admin2 level for a given set of users and speed thresholds.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    events_df : pd.DataFrame\n",
    "        Raw CDR events with columns: user_id, timestamp, cell_id\n",
    "    cells_df : pd.DataFrame\n",
    "        Tower metadata with longitude and latitude\n",
    "    target_user_ids : list\n",
    "        IDs of users whose flows should be included\n",
    "    high_speed : float\n",
    "        High speed threshold (km/h)\n",
    "    low_speed : float\n",
    "        Low speed threshold (km/h)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Origin–destination flows with columns: origin_gid, destination_gid, flow\n",
    "    \"\"\"\n",
    "    # Filter events to target users\n",
    "    target_events = events_df[events_df['user_id'].isin(target_user_ids)].copy()\n",
    "    if target_events.empty:\n",
    "        return pd.DataFrame(columns=['origin_gid', 'destination_gid', 'flow'])\n",
    "    \n",
    "    # Merge coordinates and project to meters\n",
    "    ev_proj = add_coordinates_in_meters_to_events(target_events, cells_df)\n",
    "    \n",
    "    # Compute speed\n",
    "    ev_speed = add_speed_metric(ev_proj)\n",
    "    \n",
    "    # Classify events into stay/move\n",
    "    ev_classified = determine_stay_move_from_speed(ev_speed, high_speed_threshold_kmh=high_speed, low_speed_threshold_kmh=low_speed)\n",
    "    \n",
    "    # Collapse into segments and only keep stay segments (for origin/destination coordinates)\n",
    "    segments = collapse_stay_move(ev_classified)\n",
    "    \n",
    "    # Remove segments with no duration or no coordinates (could occur if all NaN)\n",
    "    segments = segments.dropna(subset=['latitude', 'longitude'])\n",
    "    if segments.empty:\n",
    "        return pd.DataFrame(columns=['origin_gid', 'destination_gid', 'flow'])\n",
    "    \n",
    "    # Map stay segments to admin2 codes\n",
    "    segments_with_admin = map_points_to_admin2(segments, adm2_gdf)\n",
    "    \n",
    "    # Build OD trips: for each user, pair consecutive stays (origin → destination)\n",
    "    trips = []\n",
    "    for user, group in segments_with_admin.groupby('user_id'):\n",
    "        group = group.sort_values('start_time').reset_index(drop=True)\n",
    "        # iterate over consecutive pairs\n",
    "        for i in range(len(group) - 1):\n",
    "            origin_row = group.iloc[i]\n",
    "            dest_row = group.iloc[i + 1]\n",
    "            origin_gid = origin_row['ID_2']\n",
    "            dest_gid = dest_row['ID_2']\n",
    "            # Only count if both origin and destination codes are valid\n",
    "            if pd.notna(origin_gid) and pd.notna(dest_gid):\n",
    "                trips.append((origin_gid, dest_gid))\n",
    "    # Summarise flows\n",
    "    if not trips:\n",
    "        return pd.DataFrame(columns=['origin_gid', 'destination_gid', 'flow'])\n",
    "    trips_df = pd.DataFrame(trips, columns=['origin_gid', 'destination_gid'])\n",
    "    flows = trips_df.value_counts().reset_index(name='flow')\n",
    "    return flows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304e471a",
   "metadata": {},
   "source": [
    "### Compute OD flows for Method A (10/3 km/h) and Method B (20/3 km/h)\n",
    "We compute OD flows separately for two sets of speed thresholds. Once flows are available, we merge them on `(origin_gid, destination_gid)`, compute the absolute difference `(Flow_A – Flow_B)`, the signed magnitude `(Flow_B – Flow_A) / Flow_A`, and the percentage change.\n",
    "We then sort by the percentage change to identify OD pairs that are highly sensitive to the speed threshold (e.g., where |percent_change| > 30 %)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272b5823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "\n",
    "# Identify user IDs whose home district is in the selected target_admin2 list\n",
    "target_user_ids = home_join[home_join['ID_2'].isin(target_admin2)]['user_id'].unique().tolist()\n",
    "print(f'Number of users with home in target Admin2: {len(target_user_ids)}')\n",
    "\n",
    "# Compute OD flows for Method A\n",
    "od_A = compute_od_flows(events, cells, target_user_ids, high_speed=10, low_speed=3)\n",
    "od_A = od_A.rename(columns={'flow': 'flow_A'})\n",
    "print(f'Method A produced {len(od_A)} OD pairs.')\n",
    "\n",
    "# Compute OD flows for Method B\n",
    "od_B = compute_od_flows(events, cells, target_user_ids, high_speed=20, low_speed=3)\n",
    "od_B = od_B.rename(columns={'flow': 'flow_B'})\n",
    "print(f'Method B produced {len(od_B)} OD pairs.')\n",
    "\n",
    "# Merge the two tables on origin and destination\n",
    "od_merged = pd.merge(od_A, od_B, on=['origin_gid', 'destination_gid'], how='outer').fillna(0)\n",
    "\n",
    "# Compute absolute and relative differences\n",
    "# difference (A - B) as requested\n",
    "od_merged['diff_AB'] = od_merged['flow_A'] - od_merged['flow_B']\n",
    "\n",
    "# relative diff (B-A)/A  (ratio)  and convert to percent\n",
    "od_merged['rel_diff'] = np.where(\n",
    "    od_merged['flow_A'] > 0,\n",
    "    (od_merged['flow_B'] - od_merged['flow_A']) / od_merged['flow_A'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# magnitude of changes (signed ratio, before x100)\n",
    "od_merged['magnitude_of_change'] = od_merged['rel_diff']\n",
    "\n",
    "# percent change\n",
    "od_merged['pct_change'] = od_merged['rel_diff'] * 100\n",
    "\n",
    "# Attach Admin2 names instead of GIDs\n",
    "# adm2_gdf has columns: ID_2, NAME_2, geometry\n",
    "origin_lookup = (\n",
    "    adm2_gdf[['ID_2', 'NAME_2']]\n",
    "    .drop_duplicates()\n",
    "    .rename(columns={'ID_2': 'origin_gid', 'NAME_2': 'Origin Admin2'})\n",
    ")\n",
    "\n",
    "destination_lookup = (\n",
    "    adm2_gdf[['ID_2', 'NAME_2']]\n",
    "    .drop_duplicates()\n",
    "    .rename(columns={'ID_2': 'destination_gid', 'NAME_2': 'Destination Admin2'})\n",
    ")\n",
    "\n",
    "od_named = (\n",
    "    od_merged\n",
    "    .merge(origin_lookup, on='origin_gid', how='left')\n",
    "    .merge(destination_lookup, on='destination_gid', how='left')\n",
    ")\n",
    "\n",
    "#  keep only inter-district flows (Origin != Destination)\n",
    "od_named = od_named[od_named['Origin Admin2'] != od_named['Destination Admin2']].copy()\n",
    "\n",
    "#  keep only OD pairs where BOTH origin and destination are in the specified ID2 list\n",
    "allowed_ids = [4918, 4877, 4677, 4570, 4688, 4859, 4920]\n",
    "od_named = od_named[\n",
    "    od_named['origin_gid'].isin(allowed_ids) &\n",
    "    od_named['destination_gid'].isin(allowed_ids)\n",
    "].copy()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "#  Compute population density per Admin2 from shapefile + BRA_adm2_pop.csv\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 1) project to a metric CRS to compute area (m² → km²)\n",
    "adm2_area = adm2_gdf.to_crs(epsg=3857).copy()\n",
    "adm2_area['area_km2'] = adm2_area.geometry.area / 1e6\n",
    "\n",
    "# 2) join WorldPop totals (adm2_pop must have ID_2 and pop_sum)\n",
    "adm2_with_pop = adm2_area.merge(\n",
    "    adm2_pop[['ID_2', 'pop_sum']],\n",
    "    on='ID_2',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 3) compute population density\n",
    "adm2_with_pop['pop_density'] = adm2_with_pop['pop_sum'] / adm2_with_pop['area_km2']\n",
    "\n",
    "# 4) prepare lookup tables for origin and destination density\n",
    "density_origin = adm2_with_pop[['ID_2', 'pop_density']].rename(\n",
    "    columns={'ID_2': 'origin_gid', 'pop_density': 'Origin_pop_density'}\n",
    ")\n",
    "density_dest = adm2_with_pop[['ID_2', 'pop_density']].rename(\n",
    "    columns={'ID_2': 'destination_gid', 'pop_density': 'Destination_pop_density'}\n",
    ")\n",
    "\n",
    "# 5) merge density into OD table\n",
    "od_named = (\n",
    "    od_named\n",
    "    .merge(density_origin, on='origin_gid', how='left')\n",
    "    .merge(density_dest, on='destination_gid', how='left')\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "#  Filter on number of residents (home locations) > 15\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# home_counts has: ID_2, NAME_2, num_subscribers (residents)\n",
    "res_origin = home_counts[['ID_2', 'num_subscribers']].rename(\n",
    "    columns={'ID_2': 'origin_gid', 'num_subscribers': 'Origin_residents'}\n",
    ")\n",
    "res_dest = home_counts[['ID_2', 'num_subscribers']].rename(\n",
    "    columns={'ID_2': 'destination_gid', 'num_subscribers': 'Destination_residents'}\n",
    ")\n",
    "\n",
    "od_named = (\n",
    "    od_named\n",
    "    .merge(res_origin, on='origin_gid', how='left')\n",
    "    .merge(res_dest, on='destination_gid', how='left')\n",
    ")\n",
    "\n",
    "#  keep only OD pairs where BOTH origin and destination have > 15 residents\n",
    "od_named = od_named[\n",
    "    (od_named['Origin_residents'] > 15) &\n",
    "    (od_named['Destination_residents'] > 15)\n",
    "].copy()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "#  Apply minimum trip_count ≥ 100 (using Method A as baseline)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "od_named = od_named[od_named['flow_A'] >= 100].copy()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Sort and export\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Sort by percent_change (ascending = largest decreases at the top if negative)\n",
    "od_named = od_named.sort_values(by='pct_change', ascending=True)\n",
    "\n",
    "# Select and rename columns for output\n",
    "od_output = od_named[[\n",
    "    'Origin Admin2',\n",
    "    'Destination Admin2',\n",
    "    'flow_A',\n",
    "    'flow_B',\n",
    "    'diff_AB',\n",
    "    'magnitude_of_change',\n",
    "    'pct_change',\n",
    "    'Origin_pop_density',\n",
    "    'Destination_pop_density',\n",
    "    'Origin_residents',\n",
    "    'Destination_residents'\n",
    "]].rename(columns={\n",
    "    'flow_A': 'Trip count by method A',\n",
    "    'flow_B': 'Trip count by method B',\n",
    "    'diff_AB': 'difference (A-B)',\n",
    "    'magnitude_of_change': 'magnitude_of_change',   # ratio (B-A)/A\n",
    "    'pct_change': 'percent_change',                 # in %\n",
    "})\n",
    "\n",
    "# # Save to CSV\n",
    "# output_fp = 'od_comparison_admin2_methodA_B_selectedIDs_residents15_flowA100.csv'\n",
    "# od_output.to_csv(output_fp, index=False)\n",
    "# print(f'Saved OD comparison table to {output_fp}')\n",
    "\n",
    "# Show top 20 rows\n",
    "display(od_output.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6104c2bb",
   "metadata": {},
   "source": [
    "\n",
    "## 5 – Plotting Home Locations for Methods A and B\n",
    "\n",
    "In addition to comparing the origin–destination flows derived from different speed thresholds, it is often insightful to see where each subscriber’s home is estimated to be under different definitions of ‘day’ and ‘night’.  The **home location** is typically defined as the place where a subscriber spends the most nights and weekends.  Depending on how ‘work hours’ and ‘night‑time’ hours are defined, the candidate home tower can change.\n",
    "\n",
    "Two alternative sets of hour ranges are considered here:\n",
    "\n",
    "* **Method A (default)** – work hours are assumed to occur between **05:00 and 22:00**; night‑time is **22:00–05:00**.  These are the same ranges used in the main continuity‑model analysis.\n",
    "* **Method B (alternative)** – work hours are assumed to occur between **07:00 and 20:00**; night‑time is **20:00–07:00**.  This narrower definition of the working day may shift home/work assignments for some users.\n",
    "\n",
    "For each user and candidate cell we count:\n",
    "\n",
    "* the number of **distinct days with at least one event during work hours** (`work_hours_days_cnt`),\n",
    "* the number of **distinct days with at least one event during the night** (`nighttime_days_cnt`),\n",
    "* the number of **distinct weekend days** (Saturday/Sunday) (`distinct_weekend_days_count`), and\n",
    "* the total number of **distinct days with any events** (`distinct_days_count`).\n",
    "\n",
    "Locations with frequent night‑time and weekend activity are favoured as home.  The `classify_locations` helper below applies the same logic as in the *Meaningful Locations and Usual Environment* notebook but allows us to plug in different hour ranges.  After classification we map the home points for each method on top of the Admin 2 boundaries using Folium’s layer control.  This makes it easy to toggle between Method A and Method B and visually inspect any differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de877fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Home location comparison for Methods A and B ---\n",
    "\n",
    "# Ensure timestamp is datetime\n",
    "events['timestamp'] = pd.to_datetime(events['timestamp'])\n",
    "\n",
    "# Define a helper to build a summary table with arbitrary work/night ranges\n",
    "def build_user_cell_summary(events_df, work_start, work_end, night_start, night_end):\n",
    "    '''Return aggregated per-user/per-cell metrics using specified hour ranges.'''\n",
    "    def _distinct_days(dates):\n",
    "        return dates.dt.date.nunique()\n",
    "\n",
    "    grouped = events_df.groupby(['user_id', 'cell_id']).agg(\n",
    "        event_count=('user_id', 'count'),\n",
    "        distinct_days_count=('timestamp', _distinct_days),\n",
    "        distinct_weekend_days_count=('timestamp', lambda x: x[x.dt.dayofweek >= 5].dt.date.nunique()),\n",
    "        work_hours_days_cnt=('timestamp', lambda x: x[(x.dt.hour >= work_start) & (x.dt.hour < work_end)].dt.date.nunique()),\n",
    "        nighttime_days_cnt=('timestamp', lambda x: x[(x.dt.hour >= night_start) | (x.dt.hour < night_end)].dt.date.nunique())\n",
    "    ).reset_index()\n",
    "    return grouped\n",
    "\n",
    "# Classification helper adapted from the Meaningful Locations notebook\n",
    "def classify_locations(df,\n",
    "                       top_n_locations=4,\n",
    "                       home_min_night_days=14,\n",
    "                       home_min_weekend_days=2,\n",
    "                       work_min_work_hours_days=10,\n",
    "                       work_max_weekend_days=2,\n",
    "                       min_distinct_days=5,\n",
    "                       other_location_min_days=5):\n",
    "    '''Classifies locations as 'home', 'work', or 'other' based on tunable parameters.\\n    This function is identical to the one used in the Meaningful Locations notebook.'''\n",
    "    df_classified = df.copy()\n",
    "    # Rank locations per user by total distinct days\n",
    "    df_classified['rank'] = df_classified.groupby('user_id')['distinct_days_count'].rank(method='first', ascending=False)\n",
    "    is_top_location = df_classified['rank'] <= top_n_locations\n",
    "    is_significant_enough = df_classified['distinct_days_count'] >= min_distinct_days\n",
    "    is_home = (\n",
    "        (df_classified['nighttime_days_cnt'] >= home_min_night_days) &\n",
    "        (df_classified['distinct_weekend_days_count'] >= home_min_weekend_days)\n",
    "    )\n",
    "    is_work = (\n",
    "        (df_classified['work_hours_days_cnt'] >= work_min_work_hours_days) &\n",
    "        (df_classified['distinct_weekend_days_count'] <= work_max_weekend_days)\n",
    "    )\n",
    "    is_other = (df_classified['distinct_days_count'] >= other_location_min_days)\n",
    "    conditions = [is_top_location & is_significant_enough & is_home,\n",
    "                  is_top_location & is_significant_enough & is_work,\n",
    "                  is_other]\n",
    "    choices = ['home', 'work', 'other']\n",
    "    df_classified['location_type'] = np.select(conditions, choices, default='NaN')\n",
    "    return df_classified\n",
    "\n",
    "# Build per-user summaries for Method A and Method B\n",
    "summary_A = build_user_cell_summary(events, work_start=5, work_end=22, night_start=22, night_end=5)\n",
    "summary_B = build_user_cell_summary(events, work_start=7, work_end=20, night_start=20, night_end=7)\n",
    "\n",
    "# Classify locations\n",
    "classified_A = classify_locations(summary_A)\n",
    "classified_B = classify_locations(summary_B)\n",
    "\n",
    "# Extract home locations for each user (if multiple candidates exist, take the one with most night days)\n",
    "homes_A = (\n",
    "    classified_A[classified_A['location_type'] == 'home']\n",
    "    .sort_values(['user_id', 'nighttime_days_cnt', 'distinct_weekend_days_count'], ascending=[True, False, False])\n",
    "    .drop_duplicates(subset=['user_id'])\n",
    ")\n",
    "homes_B = (\n",
    "    classified_B[classified_B['location_type'] == 'home']\n",
    "    .sort_values(['user_id', 'nighttime_days_cnt', 'distinct_weekend_days_count'], ascending=[True, False, False])\n",
    "    .drop_duplicates(subset=['user_id'])\n",
    ")\n",
    "\n",
    "# Merge with cells metadata to obtain coordinates\n",
    "homes_A = homes_A.merge(cells[['cell_id', 'longitude', 'latitude']], on='cell_id', how='left')\n",
    "homes_B = homes_B.merge(cells[['cell_id', 'longitude', 'latitude']], on='cell_id', how='left')\n",
    "\n",
    "# Convert to GeoDataFrame for mapping\n",
    "homes_A_gdf = gpd.GeoDataFrame(homes_A, geometry=gpd.points_from_xy(homes_A['longitude'], homes_A['latitude']), crs='EPSG:4326')\n",
    "homes_B_gdf = gpd.GeoDataFrame(homes_B, geometry=gpd.points_from_xy(homes_B['longitude'], homes_B['latitude']), crs='EPSG:4326')\n",
    "\n",
    "# Base map centred on mean coordinate of all homes\n",
    "if not homes_A_gdf.empty:\n",
    "    centre_lat = float(homes_A_gdf['latitude'].mean())\n",
    "    centre_lon = float(homes_A_gdf['longitude'].mean())\n",
    "else:\n",
    "    centre_lat, centre_lon = 0.0, 0.0\n",
    "\n",
    "m = folium.Map(location=[centre_lat, centre_lon], zoom_start=6)\n",
    "\n",
    "# Feature group for Method A homes\n",
    "fg_A = folium.FeatureGroup(name='Home (Method A)')\n",
    "for _, row in homes_A_gdf.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        radius=3,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='blue',\n",
    "        fill_opacity=0.7,\n",
    "        popup=f\"User {row['user_id']} – Cell {row['cell_id']}\"\n",
    "    ).add_to(fg_A)\n",
    "\n",
    "# Feature group for Method B homes\n",
    "fg_B = folium.FeatureGroup(name='Home (Method B)')\n",
    "for _, row in homes_B_gdf.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        radius=3,\n",
    "        color='red',\n",
    "        fill=True,\n",
    "        fill_color='red',\n",
    "        fill_opacity=0.7,\n",
    "        popup=f\"User {row['user_id']} – Cell {row['cell_id']}\"\n",
    "    ).add_to(fg_B)\n",
    "\n",
    "# Add layers and control\n",
    "fg_A.add_to(m)\n",
    "fg_B.add_to(m)\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# Display map in notebook\n",
    "m\n",
    "\n",
    "# Optionally save to HTML\n",
    "# m.save(\"home_location_comparison_methodA_B.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
