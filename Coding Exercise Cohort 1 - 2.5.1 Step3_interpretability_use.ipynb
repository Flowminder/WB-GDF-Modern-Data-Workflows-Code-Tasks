{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8ca3c37",
   "metadata": {},
   "source": [
    "# Step 3: Interpretability & Use (Exercise notebook)\n",
    "\n",
    "In the final step of this exercise we combine the results of the data integrity (coverage) checks from **Step 1** and the analytical robustness analysis from **Step 2** to assess how reliable the OD matrix is for disaster preparedness.\n",
    "\n",
    "**Goal:** Decide how (and where) the mobility indicator can be safely used for disaster‑related decision‑making. We'll identify which districts or OD pairs have sufficient coverage and stable flows, and where strong caveats or additional data are needed.\n",
    "\n",
    "1. **Document the inputs, computations, flags and outputs.**  The inserted\n",
    "   Markdown explains what data goes in, what transformations are performed,\n",
    "   why a scaling factor is applied (because the sample includes only\n",
    "   approximately 1 000 synthetic users), and how the resulting flags and\n",
    "   quality levels should be interpreted.\n",
    "2. **Filter the analysis to a set of seven Admin 2 districts.**  Only\n",
    "   observations and flows whose home district belongs to the specified list\n",
    "   (Santana de Parnaíba, São Paulo, Mairiporã, Guarulhos, Mauá,\n",
    "   São Bernardo do Campo and Santo André) are used in coverage and OD flow\n",
    "   calculations.\n",
    "3. **Add an interactive Folium map.**  The map overlays the Admin 2 shapefile\n",
    "   with colour-coded polygons based on the `recommended_use` flag and a\n",
    "   tooltip showing detailed metrics for the selected districts; all other\n",
    "   polygons are drawn in grey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eee9083",
   "metadata": {},
   "source": [
    "## Data and Inputs\n",
    "\n",
    "This exercise uses the following inputs:\n",
    "\n",
    "- **events.csv:** synthetic call detail records (user_id, timestamp,\n",
    "  cell_id).  These records provide the sequences of network events for each\n",
    "  user.  We use them to compute speeds, classify ‘stay’ vs ‘move’ segments and\n",
    "  derive origin–destination (OD) flows between administrative areas.\n",
    "- **diaries.csv:** a pre‑computed stay/move diary for each user, including\n",
    "  home locations (`stay_type == 'home'`).  We use only the home records (with\n",
    "  longitude/latitude) to assign each user to an Admin 2 district.\n",
    "- **cells.csv:** metadata about each cell tower (cell_id with latitude and\n",
    "  longitude).  These coordinates allow us to convert event sequences into\n",
    "  metric coordinates for distance and speed calculations.\n",
    "- **BRA_adm2.shp / BRA_adm2_pop.csv:** the vector boundary of Brazil’s\n",
    "  second administrative level and a table of synthetic population data at district (Admin2) level, derived from WorldPop (`pop_sum`) per\n",
    "  Admin 2 district.  These layers are used to spatially join points to\n",
    "\n",
    "\n",
    "**Important note on scaling:** the synthetic dataset contains only about\n",
    "1 000 users.  To make the coverage ratio (`num_subscribers / pop_sum`) more\n",
    "interpretable for training purposes, we multiply the number of subscribers\n",
    "per district by a constant scaling factor (e.g. 10 000) before dividing by\n",
    "population.  This factor does **not** reflect an actual market share; when\n",
    "working with real data you should calibrate the factor appropriately.\n",
    "\n",
    "**Target districts:** all analyses below are limited to the following\n",
    "Admin 2 districts (ID and name). Only OD flows between these districts are analysed.\n",
    "\n",
    "| ID_2 | Name                     |\n",
    "|----:|-------------------------|\n",
    "| 4918 | Santana de Parnaíba     |\n",
    "| 4877 | São Paulo               |\n",
    "| 4677 | Mairiporã               |\n",
    "| 4570 | Guarulhos               |\n",
    "| 4688 | Mauá                    |\n",
    "| 4859 | São Bernardo do Campo   |\n",
    "| 4920 | Santo André             |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66961849",
   "metadata": {},
   "source": [
    "## 1 – Setup and Data Loading\n",
    "\n",
    "To evaluate interpretability, we need both coverage information (from Step 1) and OD flow sensitivity (from Step 2). This section loads the necessary datasets and recomputes the intermediate tables if they are not already available.\n",
    "\n",
    "We assume the following files are available in the working directory:\n",
    "* `events.csv`, `diaries.csv`, `cells.csv` – the raw synthetic MPD data.\n",
    "* `BRA_adm2_pop.csv` and `BRA_adm2.shp` – synthetic population data at district (Admin2) level, derived from WorldPop and administrative boundaries at the Admin 2 level.\n",
    "* The helper functions defined in Step 2 (to compute OD flows) are reproduced below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5372fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# File paths (adjust if you organise your files differently)\n",
    "events_fp = 'https://github.com/Flowminder/WB-GDF-Modern-Data-Workflows-Code-Tasks/raw/refs/heads/main/2.5.1_input_data/events.parquet'\n",
    "diaries_fp = 'https://github.com/Flowminder/WB-GDF-Modern-Data-Workflows-Code-Tasks/raw/refs/heads/main/2.5.1_input_data/diaries.csv'\n",
    "cells_fp = 'https://github.com/Flowminder/WB-GDF-Modern-Data-Workflows-Code-Tasks/raw/refs/heads/main/2.5.1_input_data/cells.csv'\n",
    "adm2_pop_fp = 'https://github.com/Flowminder/WB-GDF-Modern-Data-Workflows-Code-Tasks/raw/refs/heads/main/2.5.1_input_data/BRA_adm2_pop.csv'\n",
    "\n",
    "# Load CSV data\n",
    "events = pd.read_parquet(events_fp)\n",
    "diaries = pd.read_csv(diaries_fp)\n",
    "cells = pd.read_csv(cells_fp)\n",
    "adm2_pop = pd.read_csv(adm2_pop_fp)\n",
    "\n",
    "# Load only the Admin2 boundary from the shapefile zip using geopandas\n",
    "# The shapefile zip contains BRA_adm0.shp, BRA_adm1.shp, BRA_adm2.shp, BRA_adm3.shp.\n",
    "adm2_gdf = gpd.read_file(f\"https://github.com/Flowminder/WB-GDF-Modern-Data-Workflows-Code-Tasks/raw/refs/heads/main/2.5.1_input_data/BRA_adm2.gpkg\")\n",
    "# Ensure CRS is WGS84\n",
    "adm2_gdf = adm2_gdf.to_crs(epsg=4326)\n",
    "\n",
    "print(f'Loaded {len(events):,} events, {len(diaries):,} diary records, {len(cells):,} cells and {len(adm2_gdf):,} Admin2 polygons.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aab142",
   "metadata": {},
   "source": [
    "### RUN THIS HELPER FUNCTIONS CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1340ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions form previous steps just run this cell\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "def add_coordinates_in_meters_to_events(events_df: pd.DataFrame, cells_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge events with cell coordinates (longitude/latitude) and project them to a metric CRS (UTM).\n",
    "    Returns the events dataframe with additional columns: longitude, latitude, x, y (in metres).\n",
    "    \"\"\"\n",
    "    merged_df = events_df.merge(cells_df, on='cell_id', how='left')\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        merged_df,\n",
    "        geometry=gpd.points_from_xy(merged_df['longitude'], merged_df['latitude']),\n",
    "        crs='EPSG:4326'\n",
    "    )\n",
    "    # Project to UTM zone 23S (EPSG:31983) – appropriate for Brazil\n",
    "    gdf_projected = gdf.to_crs(epsg=31983)\n",
    "    gdf_projected['x'] = gdf_projected.geometry.x\n",
    "    gdf_projected['y'] = gdf_projected.geometry.y\n",
    "    return gdf_projected.drop(columns=['geometry'])\n",
    "\n",
    "def add_speed_metric(df: pd.DataFrame, group_by: str = 'user_id') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Computes backward-looking speed between consecutive cell changes for each user.\n",
    "    A constant speed is assigned within each stay/move segment.\n",
    "    \"\"\"\n",
    "    aux_df = df.sort_values([group_by, 'timestamp']).reset_index(drop=True)\n",
    "    aux_df['timestamp'] = pd.to_datetime(aux_df['timestamp'])\n",
    "    \n",
    "    def compute_speed(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        n = len(group)\n",
    "        if n == 0:\n",
    "            return group.assign(speed=np.nan)\n",
    "        x = group['x'].to_numpy(dtype=float)\n",
    "        y = group['y'].to_numpy(dtype=float)\n",
    "        ts = group['timestamp'].to_numpy('datetime64[ns]')\n",
    "        cell = group['cell_id'].to_numpy()\n",
    "        pos = np.arange(n)\n",
    "        # Identify the start index of each new cell segment\n",
    "        change_mask = np.r_[True, cell[1:] != cell[:-1]]\n",
    "        change_pos = pos[change_mask]\n",
    "        # For each index i, find the start of its current segment\n",
    "        idx_cur = np.searchsorted(change_pos, pos, side='right') - 1\n",
    "        cur_pos = np.full(n, -1, dtype=int)\n",
    "        mask_cur = idx_cur >= 0\n",
    "        cur_pos[mask_cur] = change_pos[idx_cur[mask_cur]]\n",
    "        # Previous segment start for each row\n",
    "        prev_idx = idx_cur - 1\n",
    "        prev_pos = np.full(n, -1, dtype=int)\n",
    "        mask_prev = prev_idx >= 0\n",
    "        prev_pos[mask_prev] = change_pos[prev_idx[mask_prev]]\n",
    "        # Compute speed only when both prev_pos and cur_pos are valid and different\n",
    "        speed = np.full(n, np.nan, dtype=float)\n",
    "        valid = (prev_pos != -1) & (cur_pos != -1) & (cur_pos != prev_pos)\n",
    "        if valid.any():\n",
    "            dt_h = (ts[cur_pos[valid]] - ts[prev_pos[valid]]) / np.timedelta64(1, 'h')\n",
    "            dist_km = np.hypot(\n",
    "                x[cur_pos[valid]] - x[prev_pos[valid]],\n",
    "                y[cur_pos[valid]] - y[prev_pos[valid]]\n",
    "            ) / 1000.0\n",
    "            speed_vals = np.full(valid.sum(), np.nan)\n",
    "            nonzero = dt_h > 0\n",
    "            speed_vals[nonzero] = dist_km[nonzero] / dt_h[nonzero]\n",
    "            speed[valid] = speed_vals\n",
    "        return group.assign(speed=speed)\n",
    "    aux_df = aux_df.groupby(group_by, group_keys=False).apply(compute_speed)\n",
    "    return aux_df\n",
    "\n",
    "def determine_stay_move_from_speed(df: pd.DataFrame, high_speed_threshold_kmh: float = 10.0, low_speed_threshold_kmh: float = 3.0, group_by: str = 'user_id') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Classify each event as 'move' or 'stay' based on speed thresholds.  \n",
    "    High threshold marks definite moves; contiguous rows with speed above the low threshold are also marked as moves.  \n",
    "    \"\"\"\n",
    "    def process_user(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        group = group.sort_values('timestamp').reset_index(drop=True)\n",
    "        group['speed'] = pd.to_numeric(group['speed'], errors='coerce')\n",
    "        # Initial classification: speed > high_speed → move\n",
    "        group['event_type'] = np.where(group['speed'] > high_speed_threshold_kmh, 'move', 'stay')\n",
    "        seed = group['event_type'].eq('move')\n",
    "        cand = group['speed'] > low_speed_threshold_kmh\n",
    "        mask = seed | cand\n",
    "        # Identify contiguous candidate islands\n",
    "        block_id = (mask.ne(mask.shift(fill_value=False))).cumsum()\n",
    "        block_id = block_id.where(mask)\n",
    "        # If any seed exists in the block, extend moves to include candidates\n",
    "        has_seed = seed.groupby(block_id).transform('any').fillna(False).astype(bool)\n",
    "        extend = cand & has_seed\n",
    "        group['event_type'] = np.where(seed | extend, 'move', 'stay')\n",
    "        return group\n",
    "    result = df.groupby(group_by, group_keys=False).apply(process_user)\n",
    "    return result\n",
    "\n",
    "def collapse_stay_move(df: pd.DataFrame, time_col: str = 'timestamp', type_col: str = 'event_type', lat_col: str = 'latitude', lon_col: str = 'longitude', group_by: str = 'user_id') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Collapse consecutive events of the same type into segments.  \n",
    "    For stay segments, compute a time-weighted centroid; for move segments, lat/lon may remain NaN.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    out[time_col] = pd.to_datetime(out[time_col], errors='coerce')\n",
    "    out = out.sort_values([group_by, time_col])\n",
    "    out['_seg_id'] = out.groupby(group_by)[type_col].transform(lambda s: (s != s.shift()).cumsum())\n",
    "    seg_bounds = out.groupby([group_by, '_seg_id']).agg(\n",
    "        event_type=(type_col, 'first'),\n",
    "        start_time=(time_col, 'first'),\n",
    "        end_time=(time_col, 'last'),\n",
    "    )\n",
    "    out = out.join(seg_bounds[['end_time']], on=[group_by, '_seg_id'], rsuffix='_seg')\n",
    "    out['_next_time'] = out.groupby([group_by, '_seg_id'])[time_col].shift(-1)\n",
    "    w = (out['_next_time'].fillna(out['end_time']) - out[time_col]).dt.total_seconds().clip(lower=0)\n",
    "    out['_w'] = w\n",
    "    out['_wlat'] = out['_w'] * out[lat_col]\n",
    "    out['_wlon'] = out['_w'] * out[lon_col]\n",
    "    agg = out.groupby([group_by, '_seg_id']).agg(\n",
    "        event_type=(type_col, 'first'),\n",
    "        start_time=(time_col, 'first'),\n",
    "        end_time=(time_col, 'last'),\n",
    "        w_sum=('_w', 'sum'),\n",
    "        wlat_sum=('_wlat', 'sum'),\n",
    "        wlon_sum=('_wlon', 'sum'),\n",
    "        lat_mean=(lat_col, 'mean'),\n",
    "        lon_mean=(lon_col, 'mean'),\n",
    "    )\n",
    "    agg['duration_s'] = (agg['end_time'] - agg['start_time']).dt.total_seconds()\n",
    "    stay_mask = agg['event_type'].eq('stay')\n",
    "    has_weight = agg['w_sum'] > 0\n",
    "    agg['latitude'] = np.where(stay_mask & has_weight, agg['wlat_sum'] / agg['w_sum'], np.where(stay_mask, agg['lat_mean'], np.nan))\n",
    "    agg['longitude'] = np.where(stay_mask & has_weight, agg['wlon_sum'] / agg['w_sum'], np.where(stay_mask, agg['lon_mean'], np.nan))\n",
    "    result = (\n",
    "        agg.reset_index(level='_seg_id', drop=True).reset_index()[\n",
    "            [group_by, 'event_type', 'start_time', 'end_time', 'duration_s', 'latitude', 'longitude']\n",
    "        ].sort_values([group_by, 'start_time']).reset_index(drop=True)\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def map_points_to_admin2(points_df: pd.DataFrame, adm2_gdf: gpd.GeoDataFrame, lat_col: str = 'latitude', lon_col: str = 'longitude') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assign each point in `points_df` to an Admin2 polygon using a spatial join.  \n",
    "    Returns a copy of the input DataFrame with added columns `GID_2` and `NAME_2`.  \n",
    "    \"\"\"\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        points_df,\n",
    "        geometry=gpd.points_from_xy(points_df[lon_col], points_df[lat_col]),\n",
    "        crs='EPSG:4326'\n",
    "    )\n",
    "    join = gpd.sjoin(gdf, adm2_gdf[['ID_2', 'NAME_2', 'geometry']], how='left', predicate='within')\n",
    "    # Return DataFrame without geometry\n",
    "    return join.drop(columns=['geometry'])\n",
    "\n",
    "\n",
    "from typing import Tuple, List\n",
    "\n",
    "def compute_od_flows(events_df: pd.DataFrame,\n",
    "                     cells_df: pd.DataFrame,\n",
    "                     target_user_ids: List[int],\n",
    "                     high_speed: float,\n",
    "                     low_speed: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute origin–destination flows at Admin2 level for a given set of users and speed thresholds.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    events_df : pd.DataFrame\n",
    "        Raw CDR events with columns: user_id, timestamp, cell_id\n",
    "    cells_df : pd.DataFrame\n",
    "        Tower metadata with longitude and latitude\n",
    "    target_user_ids : list\n",
    "        IDs of users whose flows should be included\n",
    "    high_speed : float\n",
    "        High speed threshold (km/h)\n",
    "    low_speed : float\n",
    "        Low speed threshold (km/h)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Origin–destination flows with columns: origin_gid, destination_gid, flow\n",
    "    \"\"\"\n",
    "    # Filter events to target users\n",
    "    target_events = events_df[events_df['user_id'].isin(target_user_ids)].copy()\n",
    "    if target_events.empty:\n",
    "        return pd.DataFrame(columns=['origin_gid', 'destination_gid', 'flow'])\n",
    "    \n",
    "    # Merge coordinates and project to meters\n",
    "    ev_proj = add_coordinates_in_meters_to_events(target_events, cells_df)\n",
    "    \n",
    "    # Compute speed\n",
    "    ev_speed = add_speed_metric(ev_proj)\n",
    "    \n",
    "    # Classify events into stay/move\n",
    "    ev_classified = determine_stay_move_from_speed(ev_speed, high_speed_threshold_kmh=high_speed, low_speed_threshold_kmh=low_speed)\n",
    "    \n",
    "    # Collapse into segments and only keep stay segments (for origin/destination coordinates)\n",
    "    segments = collapse_stay_move(ev_classified)\n",
    "    \n",
    "    # Remove segments with no duration or no coordinates (could occur if all NaN)\n",
    "    segments = segments.dropna(subset=['latitude', 'longitude'])\n",
    "    if segments.empty:\n",
    "        return pd.DataFrame(columns=['origin_gid', 'destination_gid', 'flow'])\n",
    "    \n",
    "    # Map stay segments to admin2 codes\n",
    "    segments_with_admin = map_points_to_admin2(segments, adm2_gdf)\n",
    "    \n",
    "    # Build OD trips: for each user, pair consecutive stays (origin → destination)\n",
    "    trips = []\n",
    "    for user, group in segments_with_admin.groupby('user_id'):\n",
    "        group = group.sort_values('start_time').reset_index(drop=True)\n",
    "        # iterate over consecutive pairs\n",
    "        for i in range(len(group) - 1):\n",
    "            origin_row = group.iloc[i]\n",
    "            dest_row = group.iloc[i + 1]\n",
    "            origin_gid = origin_row['ID_2']\n",
    "            dest_gid = dest_row['ID_2']\n",
    "            # Only count if both origin and destination codes are valid\n",
    "            if pd.notna(origin_gid) and pd.notna(dest_gid):\n",
    "                trips.append((origin_gid, dest_gid))\n",
    "    # Summarise flows\n",
    "    if not trips:\n",
    "        return pd.DataFrame(columns=['origin_gid', 'destination_gid', 'flow'])\n",
    "    trips_df = pd.DataFrame(trips, columns=['origin_gid', 'destination_gid'])\n",
    "    flows = trips_df.value_counts().reset_index(name='flow')\n",
    "    return flows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11137cd",
   "metadata": {},
   "source": [
    "## Coverage calculation\n",
    "\n",
    "The following code cell computes the coverage ratio per district for the\n",
    "target areas.  Home records are spatially joined to Admin 2 polygons,\n",
    "filtered to the seven districts listed above, and grouped by district name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c62ab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 0. Config: list of Admin2 we focus on for the exercise (7 target areas)\n",
    "#    These are used for coverage & interpretability, but OD users\n",
    "#    are selected from a larger set (top N Admin2) as in Step 2.\n",
    "# ------------------------------------------------------------------\n",
    "allowed_ids = [4918, 4877, 4677, 4570, 4688, 4859, 4920]\n",
    "allowed_names = [\n",
    "    'Santana de Parnaíba', 'São Paulo', 'Mairiporã',\n",
    "    'Guarulhos', 'Mauá', 'São Bernardo do Campo', 'Santo André'\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. Home locations for ALL Admin2 (same idea as Step 2 – no filter yet)\n",
    "# ------------------------------------------------------------------\n",
    "home_df = (\n",
    "    diaries[diaries['stay_type'] == 'home']\n",
    "    .dropna(subset=['longitude', 'latitude'])\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "home_gdf = gpd.GeoDataFrame(\n",
    "    home_df,\n",
    "    geometry=gpd.points_from_xy(home_df['longitude'], home_df['latitude']),\n",
    "    crs='EPSG:4326'\n",
    ")\n",
    "\n",
    "home_join = gpd.sjoin(\n",
    "    home_gdf,\n",
    "    adm2_gdf[['ID_2', 'NAME_2', 'geometry']],\n",
    "    how='left',\n",
    "    predicate='within'\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Count residents per Admin2 (all districts)\n",
    "# ------------------------------------------------------------------\n",
    "home_counts_all = (\n",
    "    home_join\n",
    "    .groupby(['ID_2', 'NAME_2'])['user_id']\n",
    "    .nunique()\n",
    "    .reset_index(name='num_subscribers')\n",
    ")\n",
    "\n",
    "home_counts_all = home_counts_all.sort_values('num_subscribers', ascending=False)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Define target_admin2 for OD flows\n",
    "#    -> Top N Admin2 by number of residents (same logic as Step 2)\n",
    "#    If Step 2 uses head(12), set TOP_N = 12\n",
    "# ------------------------------------------------------------------\n",
    "TOP_N = 12\n",
    "target_admin2 = home_counts_all.head(TOP_N)['ID_2'].tolist()\n",
    "\n",
    "print(\"Top Admin2 districts by number of subscribers (used for OD):\")\n",
    "display(home_counts_all.head(TOP_N))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. Build coverage table only for the 7 focus Admin2 (allowed_ids)\n",
    "#    and scale subscribers for demonstration of coverage levels.\n",
    "# ------------------------------------------------------------------\n",
    "district_cov = (\n",
    "    adm2_pop[['ID_2', 'NAME_2', 'pop_sum']]\n",
    "    .merge(home_counts_all, on=['ID_2', 'NAME_2'], how='left')\n",
    ")\n",
    "\n",
    "district_cov['num_subscribers'] = district_cov['num_subscribers'].fillna(0)\n",
    "\n",
    "# keep only our 7 focus areas\n",
    "district_cov = district_cov[district_cov['ID_2'].isin(allowed_ids)].copy()\n",
    "\n",
    "# Scale num_subscribers just for illustration of coverage_ratio\n",
    "# (we only have 1000 users; this is for demo, not real calibration)\n",
    "SCALE_FACTOR = 7500  # does NOT affect trip counts, only coverage_ratio\n",
    "\n",
    "district_cov['coverage_ratio'] = (\n",
    "    district_cov['num_subscribers'] * SCALE_FACTOR / district_cov['pop_sum']\n",
    ")\n",
    "\n",
    "def classify_cov(r):\n",
    "    if r >= 0.4:\n",
    "        return 'High'\n",
    "    elif r >= 0.2:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Low'\n",
    "\n",
    "district_cov['coverage_flag'] = district_cov['coverage_ratio'].apply(classify_cov)\n",
    "\n",
    "print(\"Coverage table (scaled, 7 focus Admin2):\")\n",
    "display(district_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e272dfeb",
   "metadata": {},
   "source": [
    "### Coverage flag definitions\n",
    "\n",
    "- **High:** the scaled coverage ratio is ≥ 0.4.  The synthetic MNO sample\n",
    "  represents at least half of the synthetic population (after scaling), so\n",
    "  population estimates are considered robust.\n",
    "- **Medium:** the ratio is between 0.2 and 0.4.  The operator’s coverage is\n",
    "  moderate; estimates can be used for rough trends but not detailed\n",
    "  planning without caution.\n",
    "- **Low:** the ratio is below 0.2.  There are too few subscribers in this\n",
    "  district, so any indicators derived from these data may be unreliable.\n",
    "\n",
    "> **Note:** These threshold values and the scaling factor are *synthetic* and\n",
    "> chosen only for this exercise so that we can clearly see High / Medium / Low\n",
    "> coverage examples. For real analyses, they must be calibrated using\n",
    "> realistic information about market share and subscriber coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b2e0af",
   "metadata": {},
   "source": [
    "## OD flows and sensitivity\n",
    "\n",
    "The next two cells compute origin–destination flows under two sets of speed\n",
    "thresholds (Method A and Method B) and quantify how sensitive each district\n",
    "is to that parameter choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b5b070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 0. Users whose home is in one of the target Admin2\n",
    "#    (same definition as Step 2: top N Admin2, not only 7 focus areas)\n",
    "# ------------------------------------------------------------------\n",
    "target_user_ids = (\n",
    "    home_join[home_join['ID_2'].isin(target_admin2)]['user_id']\n",
    "    .unique()\n",
    "    .tolist()\n",
    ")\n",
    "print(f'Number of users with home in target Admin2: {len(target_user_ids)}')\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. Compute OD flows for Method A & B (same parameters as Step 2)\n",
    "# ------------------------------------------------------------------\n",
    "od_A = compute_od_flows(\n",
    "    events, cells, target_user_ids,\n",
    "    high_speed=10, low_speed=3\n",
    ").rename(columns={'flow': 'flow_A'})\n",
    "\n",
    "od_B = compute_od_flows(\n",
    "    events, cells, target_user_ids,\n",
    "    high_speed=20, low_speed=3\n",
    ").rename(columns={'flow': 'flow_B'})\n",
    "\n",
    "print(f'Method A produced {len(od_A)} OD pairs.')\n",
    "print(f'Method B produced {len(od_B)} OD pairs.')\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Merge flows + compute differences (mirror Step 2)\n",
    "# ------------------------------------------------------------------\n",
    "od_merged = pd.merge(\n",
    "    od_A, od_B,\n",
    "    on=['origin_gid', 'destination_gid'],\n",
    "    how='outer'\n",
    ").fillna(0)\n",
    "\n",
    "# difference (A - B) and relative / percent change\n",
    "od_merged['diff_AB'] = od_merged['flow_A'] - od_merged['flow_B']\n",
    "od_merged['rel_diff'] = np.where(\n",
    "    od_merged['flow_A'] > 0,\n",
    "    (od_merged['flow_B'] - od_merged['flow_A']) / od_merged['flow_A'],\n",
    "    np.nan\n",
    ")\n",
    "od_merged['pct_change'] = od_merged['rel_diff'] * 100\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Attach Admin2 names\n",
    "# ------------------------------------------------------------------\n",
    "adm2_names = adm2_gdf[['ID_2', 'NAME_2']].drop_duplicates()\n",
    "\n",
    "origin_lookup = adm2_names.rename(\n",
    "    columns={'ID_2': 'origin_gid', 'NAME_2': 'Origin Admin2'}\n",
    ")\n",
    "destination_lookup = adm2_names.rename(\n",
    "    columns={'ID_2': 'destination_gid', 'NAME_2': 'Destination Admin2'}\n",
    ")\n",
    "\n",
    "od_named = (\n",
    "    od_merged\n",
    "    .merge(origin_lookup, on='origin_gid', how='left')\n",
    "    .merge(destination_lookup, on='destination_gid', how='left')\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. Residents per Admin2 (from home_join; same base used for coverage)\n",
    "# ------------------------------------------------------------------\n",
    "residents = (\n",
    "    home_join\n",
    "    .groupby('ID_2')['user_id']\n",
    "    .nunique()\n",
    "    .reset_index(name='num_residents')\n",
    ")\n",
    "\n",
    "res_origin = residents.rename(\n",
    "    columns={'ID_2': 'origin_gid', 'num_residents': 'Origin_residents'}\n",
    ")\n",
    "res_dest = residents.rename(\n",
    "    columns={'ID_2': 'destination_gid', 'num_residents': 'Destination_residents'}\n",
    ")\n",
    "\n",
    "od_named = (\n",
    "    od_named\n",
    "    .merge(res_origin, on='origin_gid', how='left')\n",
    "    .merge(res_dest, on='destination_gid', how='left')\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5. Apply Step 2 filtering logic EXACTLY:\n",
    "#\n",
    "#   - Only OD where both origin & destination are in allowed_ids (7 focus areas)\n",
    "#   - O != D (no intra-district)\n",
    "#   - Origin_residents > 15 and Destination_residents > 15\n",
    "#   - Trip count by Method A >= 100\n",
    "# ------------------------------------------------------------------\n",
    "mask = (\n",
    "    (od_named['origin_gid'].isin(allowed_ids)) &\n",
    "    (od_named['destination_gid'].isin(allowed_ids)) &\n",
    "    (od_named['origin_gid'] != od_named['destination_gid']) &\n",
    "    (od_named['Origin_residents'] > 15) &\n",
    "    (od_named['Destination_residents'] > 15) &\n",
    "    (od_named['flow_A'] >= 100)\n",
    ")\n",
    "\n",
    "od_filtered = od_named[mask].copy()\n",
    "od_filtered = od_filtered.sort_values('pct_change', ascending=True)\n",
    "\n",
    "od_output = od_filtered[[\n",
    "    'Origin Admin2',\n",
    "    'Destination Admin2',\n",
    "    'flow_A',\n",
    "    'flow_B',\n",
    "    'diff_AB',\n",
    "    'rel_diff',\n",
    "    'pct_change',\n",
    "    'Origin_residents',\n",
    "    'Destination_residents'\n",
    "]].rename(columns={\n",
    "    'flow_A': 'Trip count by method A',\n",
    "    'flow_B': 'Trip count by method B'\n",
    "})\n",
    "\n",
    "print(f'Filtered OD pairs (Step 3, same logic as Step 2 + 7 focus Admin2): {len(od_output)}')\n",
    "display(od_output.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07eb172",
   "metadata": {},
   "source": [
    "Now compute the sensitivity per district by looking at the maximum absolute\n",
    "percentage change across all flows touching each district (either as origin or\n",
    "destination):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5227b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Compute max absolute percent change for each district\n",
    "#    (based on pct_change = (Flow_B - Flow_A) / Flow_A * 100 )\n",
    "origin_sens = (\n",
    "    od_merged.groupby('origin_gid')['pct_change']\n",
    "    .apply(lambda x: np.nanmax(np.abs(x)))\n",
    "    .reset_index(name='max_abs_percent_change')\n",
    ")\n",
    "\n",
    "dest_sens = (\n",
    "    od_merged.groupby('destination_gid')['pct_change']\n",
    "    .apply(lambda x: np.nanmax(np.abs(x)))\n",
    "    .reset_index(name='max_abs_percent_change_dest')\n",
    ")\n",
    "\n",
    "# 6. Merge with coverage table (district_cov is already limited to the focus Admin2)\n",
    "district_sens = district_cov.merge(\n",
    "    origin_sens, left_on='ID_2', right_on='origin_gid', how='left'\n",
    ").merge(\n",
    "    dest_sens, left_on='ID_2', right_on='destination_gid', how='left'\n",
    ")\n",
    "\n",
    "# 7. Combine origin / destination sensitivity into a single value per district\n",
    "_vals = district_sens[['max_abs_percent_change', 'max_abs_percent_change_dest']].to_numpy()\n",
    "district_sens['max_abs_percent_change'] = np.nanmax(_vals, axis=1)\n",
    "\n",
    "# drop helper columns we no longer need\n",
    "district_sens = district_sens.drop(\n",
    "    columns=['max_abs_percent_change_dest', 'origin_gid', 'destination_gid']\n",
    ")\n",
    "\n",
    "# 8. Classify sensitivity level for each district\n",
    "def classify_sensitivity(x):\n",
    "    if pd.isna(x):\n",
    "        # If we never see this district in any OD pair, treat sensitivity as High (very uncertain)\n",
    "        return 'High'\n",
    "    x_abs = abs(x)\n",
    "    return 'Low' if x_abs < 20 else ('Medium' if x_abs < 40 else 'High')\n",
    "\n",
    "district_sens['sensitivity_flag'] = district_sens['max_abs_percent_change'].apply(classify_sensitivity)\n",
    "\n",
    "print(\"Coverage & sensitivity table:\")\n",
    "display(\n",
    "    district_sens[\n",
    "        ['ID_2', 'NAME_2', 'pop_sum', 'num_subscribers',\n",
    "         'coverage_ratio', 'coverage_flag',\n",
    "         'max_abs_percent_change', 'sensitivity_flag']\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1849e658",
   "metadata": {},
   "source": [
    "### Sensitivity flag definitions\n",
    "\n",
    "- **Low:** the maximum absolute percent change across OD flows touching this\n",
    "  district is less than 20 %.  The flows are stable with respect to the\n",
    "  speed-threshold parameter.\n",
    "- **Medium:** the max percent change is between 20 % and 40 %.  Flows are\n",
    "  moderately sensitive; indicators should be used with caution.\n",
    "- **High:** the max percent change is greater than 40 % or missing.  OD flows\n",
    "  vary significantly under different parameter choices; use these indicators\n",
    "  only for rough approximations.\n",
    "\n",
    "> **Note:** These sensitivity thresholds are also *synthetic* and chosen to\n",
    "> highlight clear differences between Low / Medium / High sensitivity in this\n",
    "> small 1,000-user sample. In a real deployment, the cut-offs should be tuned\n",
    "> to the context and validated against domain knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc58512",
   "metadata": {},
   "source": [
    "## Quality assessment and recommended use\n",
    "\n",
    "Combine the coverage and sensitivity flags to derive a quality level and a\n",
    "recommended use for each district:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4d11b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9️.Derive quality level\n",
    "def derive_quality(row):\n",
    "    cov = row['coverage_flag']\n",
    "    sens = row['sensitivity_flag']\n",
    "    if cov == 'High' and sens == 'Low':\n",
    "        return 'High quality'\n",
    "    elif cov in ('High', 'Medium') and sens in ('Low', 'Medium'):\n",
    "        return 'Medium quality'\n",
    "    else:\n",
    "        return 'Low quality'\n",
    "\n",
    "district_sens['quality_level'] = district_sens.apply(derive_quality, axis=1)\n",
    "\n",
    "# 10.Recommended use based on quality\n",
    "def recommend_use(q):\n",
    "    if q == 'High quality':\n",
    "        return 'Suitable for detailed evacuation planning'\n",
    "    elif q == 'Medium quality':\n",
    "        return 'Suitable only for rough prioritisation'\n",
    "    else:\n",
    "        return 'Not suitable without additional information'\n",
    "\n",
    "district_sens['recommended_use'] = district_sens['quality_level'].apply(recommend_use)\n",
    "\n",
    "print('Final quality assessment:')\n",
    "display(\n",
    "    district_sens[[\n",
    "        'ID_2','NAME_2','coverage_ratio','coverage_flag',\n",
    "        'max_abs_percent_change','sensitivity_flag',\n",
    "        'quality_level','recommended_use'\n",
    "    ]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc0061a",
   "metadata": {},
   "source": [
    "### Quality & use definitions\n",
    "\n",
    "> **Note:** The quality levels below are based on *synthetic* thresholds\n",
    "> chosen for this exercise (coverage cut-offs and sensitivity cut-offs).  \n",
    "> They are meant to illustrate how coverage and robustness might be combined.\n",
    "> In a real application, these thresholds should be calibrated using\n",
    "> domain expertise and validation against ground truth.\n",
    "\n",
    "- **High quality:**  \n",
    "  - Typically occurs when **coverage is High** and **sensitivity is Low**.  \n",
    "  - We see enough subscribers to represent the synthetic population after\n",
    "    scaling, *and* OD flows are stable when we change the speed thresholds.  \n",
    "  - The OD matrix is considered reliable for **detailed disaster planning**\n",
    "    in this district (e.g. evacuation routing, shelter allocation).\n",
    "\n",
    "- **Medium quality:**  \n",
    "  - Occurs for **intermediate combinations**, e.g. High/Medium coverage with\n",
    "    Low/Medium sensitivity.  \n",
    "  - The results capture the **main patterns**, but either sample size or\n",
    "    robustness is not strong enough for precise decisions.  \n",
    "  - Use these indicators to **set broad priorities only** (e.g. which areas\n",
    "    to look at first), not for fine-grained interventions.\n",
    "\n",
    "- **Low quality:**  \n",
    "  - Triggered when **coverage is Low** or **sensitivity is High** (flows\n",
    "    react strongly to parameter changes or are missing).  \n",
    "  - Either we see too few subscribers, or the OD estimates are highly\n",
    "    unstable under reasonable modelling choices.  \n",
    "  - These indicators **should not be used for operational planning**\n",
    "    without additional data, re-calibration of parameters, or external\n",
    "    validation (e.g. surveys, traffic counts, other operators).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa4f5fb",
   "metadata": {},
   "source": [
    "##  Folium choropleth map\n",
    "\n",
    "Finally, create an interactive map showing the `recommended_use` for the\n",
    "target districts.  Districts outside the target list are coloured grey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95965bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Colour palette for the three categories\n",
    "colour_map = {\n",
    "    'Suitable for detailed evacuation planning': '#2ca02c',  # green\n",
    "    'Suitable only for rough prioritisation': '#ff7f0e',     # orange\n",
    "    'Not suitable without additional information': '#d62728' # red\n",
    "}\n",
    "\n",
    "# Create a base map centred on the target districts\n",
    "centre_lat = home_join['latitude'].mean()\n",
    "centre_lon = home_join['longitude'].mean()\n",
    "m = folium.Map(location=[centre_lat, centre_lon], zoom_start=9, tiles='cartodbpositron')\n",
    "\n",
    "# Create a lookup for colours and tooltip info by ID_2\n",
    "info_lookup = district_sens.set_index('ID_2').to_dict(orient='index')\n",
    "\n",
    "# Define a style function for colouring polygons\n",
    "def style_function(feature):\n",
    "    gid = feature['properties']['ID_2']\n",
    "    if gid in info_lookup:\n",
    "        use = info_lookup[gid]['recommended_use']\n",
    "        return {\n",
    "            'fillColor': colour_map[use],\n",
    "            'color': 'black',\n",
    "            'weight': 0.5,\n",
    "            'fillOpacity': 0.6\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'fillColor': '#cccccc',  # grey for non-target areas\n",
    "            'color': 'white',\n",
    "            'weight': 0.5,\n",
    "            'fillOpacity': 0.4\n",
    "        }\n",
    "\n",
    "# Define a tooltip function to show details for target districts\n",
    "def tooltip_function(feature):\n",
    "    gid = feature['properties']['ID_2']\n",
    "    if gid in info_lookup:\n",
    "        info = info_lookup[gid]\n",
    "        return (\n",
    "            f\"ID_2: {gid}<br>\"\n",
    "            f\"Name: {info['NAME_2']}<br>\"\n",
    "            f\"Coverage ratio: {info['coverage_ratio']:.3f}<br>\"\n",
    "            f\"Coverage flag: {info['coverage_flag']}<br>\"\n",
    "            f\"Max abs % change: {info['max_abs_percent_change']:.1f}<br>\"\n",
    "            f\"Sensitivity flag: {info['sensitivity_flag']}<br>\"\n",
    "            f\"Quality level: {info['quality_level']}<br>\"\n",
    "            f\"Recommended use: {info['recommended_use']}\"\n",
    "        )\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Add the GeoJson layer with style and tooltips\n",
    "folium.GeoJson(\n",
    "    adm2_gdf.__geo_interface__,\n",
    "    name='Admin2',\n",
    "    style_function=style_function,\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=[],\n",
    "        aliases=[],\n",
    "        labels=False,\n",
    "        sticky=False,\n",
    "        toLocaleString=False,\n",
    "        style=(\"background-color: white; border: 1px solid black; \"\n",
    "               \"padding: 5px;\")\n",
    "    ),\n",
    "    highlight_function=lambda x: {'weight': 3, 'color': 'yellow'}\n",
    ").add_to(m)\n",
    "\n",
    "# Override the tooltip for target districts: add markers at an interior point\n",
    "for gid, info in info_lookup.items():\n",
    "    # find the feature geometry\n",
    "    poly = adm2_gdf[adm2_gdf['ID_2'] == gid].iloc[0].geometry\n",
    "\n",
    "    # use an interior point instead of centroid (centroid can fall outside for concave shapes)\n",
    "    pt = poly.representative_point()\n",
    "\n",
    "    popup_html = (\n",
    "        f\"<b>{info['NAME_2']}</b><br>\"\n",
    "        f\"Coverage ratio: {info['coverage_ratio']:.3f}<br>\"\n",
    "        f\"Coverage flag: {info['coverage_flag']}<br>\"\n",
    "        f\"Max abs % change: {info['max_abs_percent_change']:.1f}<br>\"\n",
    "        f\"Sensitivity flag: {info['sensitivity_flag']}<br>\"\n",
    "        f\"Quality level: {info['quality_level']}<br>\"\n",
    "        f\"Recommended use: {info['recommended_use']}\"\n",
    "    )\n",
    "    folium.Marker(\n",
    "        location=[pt.y, pt.x],\n",
    "        popup=folium.Popup(popup_html, max_width=300),\n",
    "        icon=folium.Icon(color='blue', icon='info-sign')\n",
    "    ).add_to(m)\n",
    "\n",
    "print('Interactive map ready:')\n",
    "m\n",
    "\n",
    "# Options save to HTML file\n",
    "# m.save('district_quality_map.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905688da",
   "metadata": {},
   "source": [
    "## 3 – Conclusions and Recommendations\n",
    "\n",
    "In this final step we combined district-level coverage ratios with method\n",
    "sensitivity metrics to assess the reliability of the OD matrix for disaster\n",
    "preparedness. The resulting table summarises, for each district, the strength\n",
    "of coverage and the degree of sensitivity to the speed thresholds.\n",
    "\n",
    "You can now use the `quality_level` and `recommended_use` columns to determine\n",
    "which districts are reliable enough for detailed evacuation planning and which\n",
    "require caution or additional data. Adjust the coverage and sensitivity\n",
    "thresholds as needed to explore different scenarios.\n",
    "\n",
    "> **Important:** In this exercise we only work with a synthetic sample of\n",
    "> 1,000 users. The coverage ratios and quality classes are therefore driven by\n",
    "> two *illustrative* choices:\n",
    "> - a scaling factor applied to the number of subscribers, and  \n",
    "> - hand-picked thresholds for coverage and sensitivity.\n",
    "> These numbers are not meant to reflect the real market share of any operator.\n",
    "\n",
    "In a real application you would:\n",
    "1. Replace the synthetic scaling by an evidence-based estimate of market share\n",
    "   (e.g. using regulatory statistics or internal subscriber counts).\n",
    "2. Re-tune the coverage and sensitivity thresholds so that the flags align with\n",
    "   expert judgement and, where possible, external validation data.\n",
    "3. Use the resulting `quality_level` and `recommended_use` fields as a\n",
    "   **screening tool**:  \n",
    "   - focus detailed evacuation planning on *High-quality* districts,  \n",
    "   - use *Medium-quality* districts only for broad prioritisation, and  \n",
    "   - treat *Low-quality* districts as “high-uncertainty areas” that require\n",
    "     additional information or alternative data sources.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
